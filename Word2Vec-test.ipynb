{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    \n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download('punkt')   \n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv( \"unlabeledTrainData1.tsv\", header=0, delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "24996\n",
      "line:\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jetbrains\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \".\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line:\n",
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jetbrains\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"...\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\Users\\jetbrains\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line:\n",
      "2000\n",
      "line:\n",
      "3000\n",
      "line:\n",
      "4000\n",
      "line:\n",
      "5000\n",
      "line:\n",
      "6000\n",
      "line:\n",
      "7000\n",
      "line:\n",
      "8000\n",
      "line:\n",
      "9000\n",
      "line:\n",
      "10000\n",
      "line:\n",
      "11000\n",
      "line:\n",
      "12000\n",
      "line:\n",
      "13000\n",
      "line:\n",
      "14000\n",
      "line:\n",
      "15000\n",
      "line:\n",
      "16000\n",
      "line:\n",
      "17000\n",
      "line:\n",
      "18000\n",
      "line:\n",
      "19000\n",
      "line:\n",
      "20000\n",
      "line:\n",
      "21000\n",
      "line:\n",
      "22000\n",
      "line:\n",
      "23000\n",
      "line:\n",
      "24000\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "y = 0\n",
    "print len(train[\"review\"])\n",
    "for review in train[\"review\"]:\n",
    "    if(y % 1000 == 0):\n",
    "        print \"line: %s\" % y\n",
    "    y += 1\n",
    "    sentences += review_to_sentences(review, tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266440\n"
     ]
    }
   ],
   "source": [
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-24 19:31:19,423 : INFO : collecting all words and their counts\n",
      "2017-05-24 19:31:19,430 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-24 19:31:19,559 : INFO : PROGRESS: at sentence #10000, processed 225818 words, keeping 17776 word types\n",
      "2017-05-24 19:31:19,617 : INFO : PROGRESS: at sentence #20000, processed 451941 words, keeping 24951 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-24 19:31:19,674 : INFO : PROGRESS: at sentence #30000, processed 671563 words, keeping 30034 word types\n",
      "2017-05-24 19:31:19,729 : INFO : PROGRESS: at sentence #40000, processed 898078 words, keeping 34352 word types\n",
      "2017-05-24 19:31:19,785 : INFO : PROGRESS: at sentence #50000, processed 1117403 words, keeping 37765 word types\n",
      "2017-05-24 19:31:19,841 : INFO : PROGRESS: at sentence #60000, processed 1338704 words, keeping 40724 word types\n",
      "2017-05-24 19:31:19,895 : INFO : PROGRESS: at sentence #70000, processed 1561868 words, keeping 43334 word types\n",
      "2017-05-24 19:31:19,953 : INFO : PROGRESS: at sentence #80000, processed 1781509 words, keeping 45720 word types\n",
      "2017-05-24 19:31:20,007 : INFO : PROGRESS: at sentence #90000, processed 2005541 words, keeping 48138 word types\n",
      "2017-05-24 19:31:20,062 : INFO : PROGRESS: at sentence #100000, processed 2227527 words, keeping 50213 word types\n",
      "2017-05-24 19:31:20,112 : INFO : PROGRESS: at sentence #110000, processed 2447452 words, keeping 52088 word types\n",
      "2017-05-24 19:31:20,170 : INFO : PROGRESS: at sentence #120000, processed 2669365 words, keeping 54126 word types\n",
      "2017-05-24 19:31:20,226 : INFO : PROGRESS: at sentence #130000, processed 2895036 words, keeping 55852 word types\n",
      "2017-05-24 19:31:20,282 : INFO : PROGRESS: at sentence #140000, processed 3107998 words, keeping 57356 word types\n",
      "2017-05-24 19:31:20,338 : INFO : PROGRESS: at sentence #150000, processed 3333300 words, keeping 59058 word types\n",
      "2017-05-24 19:31:20,391 : INFO : PROGRESS: at sentence #160000, processed 3555997 words, keeping 60625 word types\n",
      "2017-05-24 19:31:20,447 : INFO : PROGRESS: at sentence #170000, processed 3779429 words, keeping 62083 word types\n",
      "2017-05-24 19:31:20,502 : INFO : PROGRESS: at sentence #180000, processed 4000326 words, keeping 63502 word types\n",
      "2017-05-24 19:31:20,555 : INFO : PROGRESS: at sentence #190000, processed 4225345 words, keeping 64806 word types\n",
      "2017-05-24 19:31:20,611 : INFO : PROGRESS: at sentence #200000, processed 4449395 words, keeping 66088 word types\n",
      "2017-05-24 19:31:20,664 : INFO : PROGRESS: at sentence #210000, processed 4671239 words, keeping 67401 word types\n",
      "2017-05-24 19:31:20,719 : INFO : PROGRESS: at sentence #220000, processed 4895728 words, keeping 68699 word types\n",
      "2017-05-24 19:31:20,776 : INFO : PROGRESS: at sentence #230000, processed 5118866 words, keeping 69966 word types\n",
      "2017-05-24 19:31:20,832 : INFO : PROGRESS: at sentence #240000, processed 5346137 words, keeping 71173 word types\n",
      "2017-05-24 19:31:20,887 : INFO : PROGRESS: at sentence #250000, processed 5560363 words, keeping 72358 word types\n",
      "2017-05-24 19:31:20,941 : INFO : PROGRESS: at sentence #260000, processed 5780385 words, keeping 73487 word types\n",
      "2017-05-24 19:31:20,980 : INFO : collected 74215 word types from a corpus of 5919982 raw words and 266440 sentences\n",
      "2017-05-24 19:31:20,982 : INFO : Loading a fresh vocabulary\n",
      "2017-05-24 19:31:21,048 : INFO : min_count=40 retains 8306 unique words (11% of original 74215, drops 65909)\n",
      "2017-05-24 19:31:21,049 : INFO : min_count=40 leaves 5559072 word corpus (93% of original 5919982, drops 360910)\n",
      "2017-05-24 19:31:21,078 : INFO : deleting the raw counts dictionary of 74215 items\n",
      "2017-05-24 19:31:21,082 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2017-05-24 19:31:21,086 : INFO : downsampling leaves estimated 4043896 word corpus (72.7% of prior 5559072)\n",
      "2017-05-24 19:31:21,091 : INFO : estimated required memory for 8306 words and 300 dimensions: 24087400 bytes\n",
      "2017-05-24 19:31:21,118 : INFO : resetting layer weights\n",
      "2017-05-24 19:31:21,276 : INFO : training model with 4 workers on 8306 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-05-24 19:31:22,290 : INFO : PROGRESS: at 4.91% examples, 989413 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:23,296 : INFO : PROGRESS: at 10.08% examples, 1015910 words/s, in_qsize 5, out_qsize 0\n",
      "2017-05-24 19:31:24,296 : INFO : PROGRESS: at 15.06% examples, 1012067 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:25,301 : INFO : PROGRESS: at 20.13% examples, 1013698 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:26,302 : INFO : PROGRESS: at 25.11% examples, 1012641 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:27,305 : INFO : PROGRESS: at 30.19% examples, 1013621 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:28,311 : INFO : PROGRESS: at 35.19% examples, 1013350 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:29,318 : INFO : PROGRESS: at 40.33% examples, 1015307 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:30,321 : INFO : PROGRESS: at 45.40% examples, 1016384 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:31,325 : INFO : PROGRESS: at 50.45% examples, 1016330 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-24 19:31:32,325 : INFO : PROGRESS: at 55.46% examples, 1015918 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:33,329 : INFO : PROGRESS: at 60.46% examples, 1014900 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:34,335 : INFO : PROGRESS: at 65.43% examples, 1013984 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:35,341 : INFO : PROGRESS: at 70.58% examples, 1015544 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-24 19:31:36,349 : INFO : PROGRESS: at 75.72% examples, 1016745 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:37,354 : INFO : PROGRESS: at 80.81% examples, 1016980 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:38,365 : INFO : PROGRESS: at 85.91% examples, 1017052 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:39,369 : INFO : PROGRESS: at 90.98% examples, 1017291 words/s, in_qsize 6, out_qsize 0\n",
      "2017-05-24 19:31:40,380 : INFO : PROGRESS: at 95.95% examples, 1016177 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-24 19:31:41,131 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-24 19:31:41,137 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-24 19:31:41,140 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-24 19:31:41,144 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-24 19:31:41,145 : INFO : training on 29599910 raw words (20218316 effective words) took 19.9s, 1018032 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 160   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-24 19:31:45,927 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-24 19:31:46,038 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2017-05-24 19:31:46,040 : INFO : not storing attribute syn0norm\n",
      "2017-05-24 19:31:46,043 : INFO : not storing attribute cum_table\n",
      "2017-05-24 19:31:46,147 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-24 19:31:41,155 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-24 19:31:51,065 : WARNING : vectors for words set(['austria']) are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'woman', 0.6395095586776733),\n",
       " (u'boy', 0.6255266666412354),\n",
       " (u'doctor', 0.6172419786453247),\n",
       " (u'soldier', 0.6160985231399536),\n",
       " (u'cop', 0.5751296281814575),\n",
       " (u'scientist', 0.5625811219215393),\n",
       " (u'businessman', 0.5614330768585205),\n",
       " (u'lady', 0.5572729110717773),\n",
       " (u'guy', 0.5551478266716003),\n",
       " (u'person', 0.5418464541435242)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'princess', 0.8047722578048706),\n",
       " (u'aunt', 0.7661514282226562),\n",
       " (u'bride', 0.7654191851615906),\n",
       " (u'sophie', 0.7429999113082886),\n",
       " (u'widow', 0.7419686317443848),\n",
       " (u'victoria', 0.7381371259689331),\n",
       " (u'ann', 0.7119592428207397),\n",
       " (u'femme', 0.7118977904319763),\n",
       " (u'elizabeth', 0.7108350396156311),\n",
       " (u'nun', 0.7031704187393188)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'terrible', 0.8109188079833984),\n",
       " (u'horrible', 0.8041103482246399),\n",
       " (u'atrocious', 0.7122700214385986),\n",
       " (u'dreadful', 0.7075834274291992),\n",
       " (u'laughable', 0.7063020467758179),\n",
       " (u'lame', 0.6759577393531799),\n",
       " (u'pathetic', 0.6611124277114868),\n",
       " (u'bad', 0.6580395698547363),\n",
       " (u'amateurish', 0.6485338807106018),\n",
       " (u'horrendous', 0.6412642002105713)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.50569500e-02,   9.38825607e-02,   1.98630095e-02,\n",
       "         5.87078594e-02,  -5.75170256e-02,  -4.14183224e-03,\n",
       "         4.93640155e-02,   1.46144824e-02,   6.17245138e-02,\n",
       "         4.84266318e-02,  -2.91249584e-02,   5.47946291e-03,\n",
       "         1.51135445e-01,  -2.22954825e-02,   4.33592536e-02,\n",
       "        -6.61959201e-02,   1.73671376e-02,   5.40489890e-02,\n",
       "         4.44530249e-02,   1.23167880e-01,   5.67670316e-02,\n",
       "        -1.92467757e-02,  -5.59160672e-02,  -9.72371250e-02,\n",
       "        -4.79278862e-02,   5.89737147e-02,   7.31465369e-02,\n",
       "         4.14881222e-02,   9.73541569e-03,  -3.57583873e-02,\n",
       "         8.78614001e-03,  -6.01675734e-02,  -1.74424481e-02,\n",
       "        -8.02147090e-02,   1.60583537e-02,   9.05642435e-02,\n",
       "         3.74523434e-03,  -2.48872135e-02,  -1.79199856e-02,\n",
       "        -1.12637408e-01,   1.08669341e-01,  -4.69968393e-02,\n",
       "        -8.93272832e-02,   3.11536132e-03,   6.28254563e-02,\n",
       "         6.59570694e-02,  -7.91858435e-02,  -3.38165946e-02,\n",
       "         1.74626932e-02,  -9.87426862e-02,  -2.56768148e-02,\n",
       "        -3.93781736e-02,  -2.10312475e-02,   1.12121953e-02,\n",
       "         8.41170549e-02,  -1.44140178e-03,  -5.41750081e-02,\n",
       "         2.79838638e-03,   3.45119350e-02,   4.36278358e-02,\n",
       "        -8.50802939e-03,   2.23415177e-02,  -9.81310159e-02,\n",
       "        -1.21934593e-01,   3.67362536e-02,  -1.31880119e-02,\n",
       "         6.25210181e-02,   2.93252571e-03,  -1.17316775e-01,\n",
       "         1.00598186e-02,   4.22559567e-02,  -6.26740828e-02,\n",
       "         5.17770350e-02,  -6.15819767e-02,   5.93355112e-02,\n",
       "        -2.88894363e-02,   4.09382544e-02,   3.72430198e-02,\n",
       "        -7.53451884e-02,  -4.88652708e-03,  -3.54059860e-02,\n",
       "         4.17958508e-05,  -1.49662504e-02,   5.16202860e-02,\n",
       "         1.36376275e-02,   4.66887653e-02,   1.05623230e-01,\n",
       "         6.49295673e-02,  -2.13401988e-02,  -8.05487409e-02,\n",
       "        -5.62629604e-04,  -2.02579666e-02,   6.98232651e-02,\n",
       "        -3.91520970e-02,  -2.11133789e-02,  -1.78184249e-02,\n",
       "        -1.08676152e-02,   2.68798433e-02,  -7.51645193e-02,\n",
       "        -1.25720669e-02,   7.07667768e-02,  -7.19645843e-02,\n",
       "        -2.89822184e-02,  -6.22061789e-02,   6.40624985e-02,\n",
       "         1.07061550e-01,  -4.15395349e-02,   1.18896775e-02,\n",
       "         9.02220421e-03,   3.44204456e-02,   1.16822580e-02,\n",
       "        -3.36321034e-02,  -1.20034544e-02,   6.27105981e-02,\n",
       "        -1.23401724e-01,   6.33763000e-02,   1.89716239e-02,\n",
       "        -4.20170911e-02,   8.38864688e-03,  -1.83831640e-02,\n",
       "         1.14584947e-02,  -4.49055992e-02,  -3.88513245e-02,\n",
       "         4.69486578e-04,  -1.90572329e-02,   9.96785462e-02,\n",
       "        -3.16309743e-03,   9.07521248e-02,  -4.26373295e-02,\n",
       "         8.72444659e-02,   1.33131109e-02,   8.84141028e-03,\n",
       "        -8.74766894e-03,  -1.92017723e-02,   7.82788470e-02,\n",
       "         3.34332623e-02,   5.47125712e-02,   1.37157943e-02,\n",
       "         4.62099165e-03,   5.05363382e-02,   9.09079090e-02,\n",
       "         2.13525798e-02,  -1.17725506e-01,  -5.65329641e-02,\n",
       "         6.17535114e-02,   4.96399254e-02,  -1.44607097e-01,\n",
       "        -8.49641263e-02,  -1.77663453e-02,   1.82663817e-02,\n",
       "         2.36648526e-02,   9.65325683e-02,  -4.88660932e-02,\n",
       "         7.26053417e-02,  -2.96301581e-03,   1.62561715e-03,\n",
       "        -2.72031929e-02,   6.60269186e-02,  -3.35450061e-02,\n",
       "         1.70696564e-02,   1.39191113e-02,  -6.18625097e-02,\n",
       "         6.67019039e-02,   8.01104978e-02,  -6.89019039e-02,\n",
       "         1.06418975e-01,  -3.75748798e-02,   3.39519866e-02,\n",
       "         4.21060696e-02,  -5.64070717e-02,  -4.55481857e-02,\n",
       "         6.33717179e-02,  -7.54387602e-02,  -8.74597654e-02,\n",
       "         9.69066098e-02,  -3.72498259e-02,  -1.52934780e-02,\n",
       "         1.01490051e-01,  -1.68632101e-02,  -5.81544861e-02,\n",
       "        -6.43269718e-03,   5.62557541e-02,  -6.37391880e-02,\n",
       "         6.92146495e-02,   7.33090639e-02,  -1.50143402e-02,\n",
       "         4.18884866e-02,  -8.91649909e-03,   1.15110792e-01,\n",
       "        -7.89161697e-02,  -9.64515954e-02,   3.99502888e-02,\n",
       "        -1.12091698e-01,   2.80552451e-02,  -8.28050636e-03,\n",
       "         2.45536584e-02,   5.10536470e-02,   1.36823831e-02,\n",
       "        -2.39730291e-02,   8.59732181e-02,  -3.60233858e-02,\n",
       "         1.22426087e-02,   3.67305316e-02,  -3.88124911e-03,\n",
       "         4.11590897e-02,   2.64384393e-02,   4.90680279e-04,\n",
       "        -1.13069482e-01,  -2.30850950e-02,  -1.67874824e-02,\n",
       "         8.59699547e-02,   5.41356541e-02,   5.05966172e-02,\n",
       "        -7.08727017e-02,   4.08107154e-02,  -1.54124433e-02,\n",
       "        -7.46129313e-03,  -1.69516131e-02,   1.02652209e-02,\n",
       "        -1.31186452e-02,   8.27742293e-02,  -3.82120609e-02,\n",
       "        -9.64265615e-02,   2.04217993e-02,  -5.59893325e-02,\n",
       "        -5.39460257e-02,   2.73169279e-02,   2.72953585e-02,\n",
       "        -9.60096642e-02,   2.53638681e-02,   4.32503829e-03,\n",
       "         6.21331744e-02,  -3.79984863e-02,   1.08021103e-01,\n",
       "        -3.42503190e-02,  -9.90224779e-02,  -3.60090397e-02,\n",
       "        -1.24679297e-01,   7.21532037e-04,   1.32192135e-01,\n",
       "        -3.17302048e-02,   8.49348214e-03,  -1.58189889e-03,\n",
       "        -3.37652899e-02,   5.71574271e-03,  -9.30632353e-02,\n",
       "         1.67931360e-03,   1.39409283e-04,  -5.12672327e-02,\n",
       "        -7.02853352e-02,   6.24895878e-02,  -6.56145141e-02,\n",
       "        -1.03917152e-01,  -2.24805679e-02,  -7.97220320e-02,\n",
       "        -2.71719899e-02,  -1.80873238e-02,   4.25066240e-03,\n",
       "         8.02336261e-03,   2.83917915e-02,  -7.78457820e-02,\n",
       "        -5.73741607e-02,   1.21756140e-02,   2.35573165e-02,\n",
       "        -1.73167642e-02,  -3.49362306e-02,   1.37233913e-01,\n",
       "         1.29464436e-02,  -7.80197829e-02,   2.39441395e-02,\n",
       "        -2.99602072e-03,  -4.46740389e-02,  -4.49719280e-03,\n",
       "        -4.18167561e-02,   3.66733782e-02,   1.33589908e-01,\n",
       "        -1.63390823e-02,  -5.91537915e-02,   6.10489324e-02,\n",
       "         1.37928918e-01,  -1.07409311e-02,  -4.76829335e-02,\n",
       "        -3.10767721e-02,  -3.13998945e-02,   1.74951237e-02,\n",
       "        -1.39840376e-02,   5.92088280e-03,  -1.37485251e-01,\n",
       "        -3.90834808e-02,   1.07798748e-01,  -2.77067497e-02,\n",
       "         1.50190108e-02,  -3.07510309e-02,  -5.62923662e-02,\n",
       "        -1.33194044e-01,   3.12342402e-02,   1.14952342e-03,\n",
       "        -6.78905025e-02,   1.17524546e-02,   2.89732739e-02], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model['awful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Получены точки слов\n"
     ]
    }
   ],
   "source": [
    "print \"Получены точки слов\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16232468,  0.15843681, -0.10505293, ..., -0.01522027,\n",
       "         0.037686  ,  0.03744352],\n",
       "       [ 0.15215264,  0.018271  ,  0.07339182, ..., -0.02757786,\n",
       "         0.03397735, -0.11259236],\n",
       "       [ 0.03003093,  0.0047795 , -0.11992312, ..., -0.04843698,\n",
       "         0.07112975, -0.07356728],\n",
       "       ..., \n",
       "       [ 0.13182379,  0.03662432, -0.03617692, ...,  0.0137189 ,\n",
       "        -0.11438576,  0.09479734],\n",
       "       [ 0.04299678,  0.06154632, -0.06060285, ...,  0.09284201,\n",
       "        -0.04943876,  0.03279798],\n",
       "       [ 0.02506608,  0.09877607, -0.02216223, ...,  0.08128311,\n",
       "        -0.04876532,  0.04317164]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
